###################### QUAL MODELO CLASSIFICA MELHOR ?ü§ñüíª ###################
#Utilizando uma base de Churn de clientes do Kaggle vamos utilizar 5 modelos
#para avaliar qual prev√™ melhor a classe correta, os modelos s√£o:
#Regress√£o log√≠stica
#√Årvore de decis√£o
#Random Forest
#AdaBoost
#XGBoost
#
#---------------------------------------------------------------------------#

#bibliotecas
library(readxl)
library(tidyverse)
library(tidymodels)
library(themis)
library(ranger)
library(xgboost)
library(gbm)
library(adabag)
library(vip)
library(C50) #pacote para modelo adaboost
library(lightgbm) #modelo lightgbm sera construido fora do tidymodels


customer_churn_dataset <- read_excel("C:/Users/m-hen/Downloads/customer_churn_dataset.xlsx", 
                                     col_types = c("numeric", "numeric", "numeric", 
                                                   "numeric", "text", "text", "text", 
                                                   "text", "text", "numeric", "text"))
View(customer_churn_dataset)
str(customer_churn_dataset)

#Ajuste da base de dados, transformando a coluna Churn em fator e retirando ID
set.seed(123)
df <- customer_churn_dataset |> 
  mutate(
    churn = factor(churn, levels = c("Yes", "No"))
  ) |> 
  select(-customer_id)

#Fazendo a separa√ß√£o entre treino e teste, 80% treino e 20% teste
split <- initial_split(df, prop = 0.80, strata = churn)
train <- training(split)
test <- testing(split)

#Criando fold para valida√ß√£o cruzada
folds <- vfold_cv(train, v = 5, strata = churn)

#Cria√ß√£o do recipe (pipeline de pr√©-processamento) do tidymodels
churn_recipe <- recipe(churn ~ ., data = train) |>  #Define alvo e preditores
  step_dummy(all_nominal_predictors()) |>  #Converte categ√≥ricas em dummies (one-hot encoding)
    step_smote(churn) |>  #Balancea as categorias "No" e "Yes" em Churn com a t√©cnica SMOTE
      step_zv(all_predictors()) |>  #Remove colunas sem vari√¢ncia
        step_normalize(all_numeric_predictors()) #Padroniza/Normaliza vari√°veis num√©ricas
#Isso coloca todas as vari√°veis na mesma escala, o que √© essencial para modelos sens√≠veis a magnitude

#Verificando o balanceamento da base treino
train |> 
  count(churn) |> 
    mutate(
      prop = n/sum(n)
    )
#Desbalanceamento moderado: 65,8% clientes que n√£o deram churn / 34,2% clientes que deram churn
#Apliquei a t√©cnica SMOTE (Synthetic Minority Oversampling Technique)
#Ela cria novas observa√ß√µes sint√©ticas da classe minorit√°ria usando interpola√ß√£o entre vizinhos pr√≥ximos

#M√©tricas que ser√£o utilizadas
metrics <- metric_set(roc_auc, accuracy, sens, spec)
#roc_auc ‚Üí mede a capacidade do modelo de separar as classes
#accuracy ‚Üí porcentagem de previs√µes corretas
#sens (sensibilidade / recall) ‚Üí capacidade de detectar a classe positiva (‚ÄúYes‚Äù)
#spec (especificidade) ‚Üí capacidade de detectar a classe negativa (‚ÄúNo‚Äù)

#Especificando os modelos dentro do tidymodels

#Regress√£o Logistica
log_model <- logistic_reg() |> 
  set_engine("glm") |> 
    set_mode("classification")

#√Årvore de deciss√£o
tree_model <- decision_tree() |> 
  set_engine("rpart") |> 
    set_mode("classification")

#Random Forest
rf_model <- rand_forest(trees = 500) |> 
  set_engine("ranger", importance = "impurity") |> 
    set_mode("classification")

#AdaBoost
ada_c50_model <- decision_tree() |> 
  set_engine("C5.0", trials = 100) |>    # trials = boosting rounds
  set_mode("classification")

#XGBoost
xgb_model <- boost_tree(
  trees = 800,
  tree_depth = 6,
  learn_rate = 0.05,
  mtry = 5,
  loss_reduction = 0,
  sample_size = 0.8
) |> 
  set_engine("xgboost") |> 
    set_mode("classification")

#Workflows
make_wf <- function(model) {
  workflow() |> 
    add_model(model) |> 
      add_recipe(churn_recipe)
} #Fun√ß√£o para rodar os modelos de maneira mais rapida

wf_log <- make_wf(log_model)
wf_tree <- make_wf(tree_model)
wf_rf <- make_wf(rf_model)
wf_ada <- make_wf(ada_c50_model)
wf_xgb <- make_wf(xgb_model)

#Treinamento
res_log <- fit_resamples(wf_log, folds, metrics = metrics, control = control_resamples(save_pred = TRUE))
res_tree <- fit_resamples(wf_tree, folds, metrics = metrics, control = control_resamples(save_pred = TRUE))
res_rf <- fit_resamples(wf_rf, folds, metrics = metrics, control = control_resamples(save_pred = TRUE))
res_ada <- fit_resamples(wf_ada, folds, metrics = metrics, control = control_resamples(save_pred = TRUE))
res_xgb <- fit_resamples(wf_xgb, folds, metrics = metrics, control = control_resamples(save_pred = TRUE))

#Previs√µes

pred_log <- collect_predictions(res_log)
pred_tree <- collect_predictions(res_tree)
pred_rf   <- collect_predictions(res_rf)
pred_ada  <- collect_predictions(res_ada)
pred_xgb  <- collect_predictions(res_xgb)

#Matriz de confus√£o
conf_mat(pred_rf, truth = churn, estimate = .pred_class)
conf_mat(pred_rf, truth = churn, estimate = .pred_class) |> 
  autoplot(type = "heatmap")


#Para ver as outras matrizes basta alterar "pred_log"

#M√©tricas
results <- bind_rows(
  collect_metrics(res_log) |> mutate(model = "Regress√£o Logistica"),
  collect_metrics(res_tree) |> mutate(model = "Arvore de Decis√£o"),
  collect_metrics(res_rf) |> mutate(model = "Random Forest"),
  collect_metrics(res_ada) |> mutate(model = "AdaBoost"),
  collect_metrics(res_xgb) |> mutate(model = "XGBoost")
) |> 
  select(model, .metric, mean, std_err) |> 
  arrange(desc(.metric == "roc_auc"), desc(mean))

results
#‚ÄúOs modelos baseados em boosting (XGBoost e AdaBoost) apresentaram o melhor desempenho discriminat√≥rio, com ROC AUC de aproximadamente 0,81. 
#O Random Forest apresentou maior acur√°cia e especificidade, sendo mais conservador na classifica√ß√£o. 
#A Regress√£o Log√≠stica apresentou maior sensibilidade, sendo mais eficiente na identifica√ß√£o de clientes propensos ao churn, por√©m com maior taxa de falsos positivos. 
#Assim, a escolha do modelo dependeria do trade-off entre recall e precis√£o desejado pelo neg√≥cio.‚Äù

#Visualiza√ß√µes
#1 - Curva ROC (como estamos trabalhando com dados desbalanceados a curva roc nesse caso n√£o interessa muito)
roc_curve(pred_log, truth = churn, .pred_Yes) |> autoplot()


#2 - Curva de Precision - Recall
# -> Foca em precision e recall
# -> Mostra como o modelo se comporta nos casos positivos
pr_curve(pred_xgb, truth = churn, .pred_Yes) |> autoplot()

#3 - Distribui√ß√£o das probabilidades previstas
# -> Mostra se o modelo separa bem as classes
pred_xgb |> ggplot(aes(.pred_Yes, fill = churn))+
  geom_density(alpha = 0.4)

#4 - Tabela de threshold (cutoff)
#Mostra como m√©tricas mudam conforme o threshold varia. √ötil para escolher o melhor ponto de corte.
threshold_perf(pred_log, truth = churn, .pred_Yes)

#5 - Ganho acumulado / Lift Chart
# -> Mostra quanto o modelo melhora a sele√ß√£o de positivos
gain_curve(pred_ada, truth = churn, .pred_Yes) |>  autoplot()

# Modelo final -> XGBoost
# Treino do Modelo no conjunto de treino, fora da valida√ß√£o cruzada
final_fit <- fit(wf_xgb, data = train)

#Previs√µes no conjunto de teste
pred_test <- predict(final_fit, new_data = test, type = "prob") |> 
  bind_cols(predict(final_fit, new_data = test, type = "class")) |> 
  bind_cols(test |> select(churn))

#M√©tricas de classifica√ß√£o somente para o teste
test_metrics <- metric_set(accuracy, sens, spec) #conjunto de m√©tricas s√≥ para o teste
test_metrics(pred_test, truth = churn, estimate = .pred_class)

#√Årea sob a curva ROC
pred_test |>  roc_auc(truth = churn, .pred_Yes, event_level = "first")

#Matriz de confus√£o
conf_mat(pred_test, truth = churn, estimate = .pred_class)

#üìä Leitura das m√©tricas
#‚úÖ 1. Accuracy = 0.843
#O modelo acerta 84.3% das previs√µes no conjunto de teste.
#Parece bom, mas accuracy engana quando h√° desbalanceamento (como churn).
#Por isso, as m√©tricas mais importantes s√£o sensibilidade e especificidade.

#üéØ 2. Sensibilidade (sens) = 0.665
#Sensibilidade mede:
  
# valor: 66.5%
#Interpreta√ß√£o:
#O modelo identifica 2 de cada 3 churns reais.
#Isso √© razo√°vel, especialmente se a classe positiva for pequena.
#Em churn, sensibilidade costuma ser mais importante que accuracy.

# 3. Especificidade (spec) = 0.935
#Especificidade mede:
  
# valor: 93.5%
#Interpreta√ß√£o:
#O modelo quase n√£o gera falsos positivos.
#Ele √© muito bom em reconhecer clientes que n√£o v√£o churnar.

#üß† O que isso significa no contexto de churn
#Seu modelo est√°:
#‚Ä¢ 	Muito bom em identificar quem N√ÉO vai churnar (spec alta)
#‚Ä¢ 	Razo√°vel em identificar quem VAI churnar (sens moderada)
#‚Ä¢ 	Globalmente bom (accuracy alta)
#Isso √© t√≠pico de modelos treinados em bases desbalanceadas, onde a classe ‚ÄúNo‚Äù √© muito mais frequente.
#

#Usando Tune_Grid() para melhorar o modelo
#Abaixo fazemos os ajustes dos hiperpar√¢metros a serem tunados
xgb_model <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  mtry = tune(),
  loss_reduction = tune()
) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#Workflow
wf_xgb_tune <- workflow() |> 
  add_model(xgb_model) |> 
  add_recipe(churn_recipe)

#Grade de Hiperpar√¢metros
grid <- grid_space_filling(
  finalize(mtry(), train),
  trees(),                         
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction(),
  size = 20)

#Execu√ß√£oüîÅ
tuned_xgb <- tune_grid(
  wf_xgb_tune,
  resamples = folds,     # valida√ß√£o cruzada
  grid = grid,
  metrics = metric_set(roc_auc, accuracy, sens, spec),
  control = control_grid(save_pred = TRUE)
)

#üèÜ Escolha dos melhores hiperpar√¢metros
best_params <- select_best(tuned_xgb, metric = "sens")
best_params

#üîç Interpreta√ß√£o dos melhores hiperpar√¢metros

#üå≥ 1. 
#O modelo est√° usando 5 vari√°veis por split.
# ‚Ä¢ 	Isso reduz correla√ß√£o entre √°rvores
# ‚Ä¢ 	Ajuda a evitar overfitting
# ‚Ä¢ 	√â um valor comum quando se busca sensibilidade (recall)
# 
# üå≤ 2. 
# Um n√∫mero alto de √°rvores.
# Isso faz sentido porque:
# ‚Ä¢ 	estou usando um learning rate extremamente baixo
# ‚Ä¢ 	ent√£o o modelo precisa de muitas √°rvores para aprender
# ‚Ä¢ 	isso tende a melhorar recall, porque o modelo vai ‚Äúlapidando‚Äù lentamente os padr√µes da classe minorit√°ria
# 
# üß† 3. 
# √Årvores profundas.
# ‚Ä¢ 	√Årvores profundas capturam intera√ß√µes complexas
# ‚Ä¢ 	Isso ajuda a identificar padr√µes raros (como churn)
# ‚Ä¢ 	Mas aumenta risco de overfitting ‚Äî que √© compensado pelo learning rate min√∫sculo
# 
# üê¢ 4.  learn_rate -> (1e-10)
# Um learning rate t√£o baixo significa:
# ‚Ä¢ 	cada √°rvore contribui quase nada
# ‚Ä¢ 	o modelo precisa de muitas √°rvores
# ‚Ä¢ 	o aprendizado √© extremamente lento
# ‚Ä¢ 	isso pode melhorar recall, mas tamb√©m pode indicar que o espa√ßo de busca encontrou um ‚Äúcanto‚Äù estranho
# Esse valor √© suspeito ‚Äî n√£o errado, mas incomum.
# Pode indicar:
# ‚Ä¢ 	a grade de hiperpar√¢metros est√° muito ampla
# ‚Ä¢ 	o modelo est√° tentando compensar overfitting
# ‚Ä¢ 	a m√©trica sensibilidade est√° favorecendo combina√ß√µes extremas
# 
# üîß 5. loss_reduction -> 0.000113
# Esse √© o gamma do XGBoost.
# ‚Ä¢ 	Valores baixos permitem splits mais agressivos
# ‚Ä¢ 	Isso aumenta sensibilidade
# ‚Ä¢ 	Ajuda a capturar padr√µes da classe minorit√°ria
# 
# üéØ Resumo da interpreta√ß√£o
# O modelo est√°:
# ‚Ä¢ 	usando muitas √°rvores
# ‚Ä¢ 	com aprendizado extremamente lento
# ‚Ä¢ 	√°rvores profundas
# ‚Ä¢ 	splits agressivos
# ‚Ä¢ 	e poucas vari√°veis por split
# Esse conjunto tende a:
# ‚Ä¢ 	aumentar sensibilidade (meu objetivo)
# ‚Ä¢ 	aumentar recall da classe positiva
# ‚Ä¢ 	mas pode reduzir precis√£o
# ‚Ä¢ 	e pode aumentar tempo de treino

# Possiveis solu√ß√µes
# 1) Ver outras combina√ß√µes de modelos
show_best(tuned_xgb, metric = "sens", n = 10)
# üéØ 1) O que o  est√° dizendo
# A melhor combina√ß√£o encontrada tem:
# ‚Ä¢ 	sensibilidade = 1.00 (perfeita)
# ‚Ä¢ 	erro padr√£o = 0
# ‚Ä¢ 	hiperpar√¢metros extremamente extremos (learning rate absurdamente baixo)
# Isso √© um sinal claro de:
# üëâ Overfitting dentro da valida√ß√£o cruzada
# O modelo encontrou uma combina√ß√£o que memoriza padr√µes da classe positiva nos folds, mas isso n√£o generaliza.
# Por√©m no conjunto de teste, a sensibilidade n√£o chega nem perto de 1.00.
# 
# üîç 2) Por que isso acontece?
# Os hiperpar√¢metros das melhores linhas:
# Linha 1 (sens = 1.00)
# Linha 2 (sens = 0.973)
# Linha 3 (sens = 0.732)
# Esses padr√µes mostram:
# ‚úîÔ∏è O modelo est√° explorando regi√µes extremas do espa√ßo de hiperpar√¢metros
# ‚Ä¢ 	learning rate muito baixo
# ‚Ä¢ 	√°rvores muito profundas
# ‚Ä¢ 	ou at√© uma √∫nica √°rvore (linha 3)
# Isso √© t√≠pico quando:
# ‚Ä¢	a grade √© muito ampla
# ‚Ä¢ 	a m√©trica favorece recall a qualquer custo
# ‚Ä¢ 	a classe positiva √© pequena
# ‚Ä¢ 	o modelo tenta ‚Äúmemorizar‚Äù os churns nos folds
# 
# ‚ö†Ô∏è 3) O maior alerta: sens = 1.00 com std_err = 0
# Isso significa:
# ‚Ä¢ 	em todos os 5 folds, a sensibilidade foi 1.00
# ‚Ä¢ 	isso √© extremamente improv√°vel em churn real
# ‚Ä¢ 	indica que o modelo est√° decorando os padr√µes da classe positiva nos folds
# Esse tipo de solu√ß√£o que n√£o generaliza.
# 
# üß† 4) Outras linhas
# Sensibilidade cai rapidamente:
# ‚Ä¢ 	0.973
# ‚Ä¢ 	0.732
# ‚Ä¢ 	0.693
# ‚Ä¢ 	0.692
# ‚Ä¢ 	0.690
# ‚Ä¢ 	0.675
# ‚Ä¢ 	0.674
# ‚Ä¢ 	0.673
# Isso mostra que:
# a maior parte das combina√ß√µes est√° em torno de 0.67‚Äì0.73
# esses valores s√£o muito mais realistas
# a solu√ß√£o com sens = 1.00 √© um outlier causado por hiperpar√¢metros extremos
#Vou rodar novamente o modelo agora com um intervalo de learn-rate e tree_depth
#learn_rate(range = c(-5, -1))  # 1e-5 a 1e-1
#tree_depth(range = c(2, 10))

#Reexecu√ß√£o do processo de tunagem do modelo agora com intervalosüîÅ
xgb_model <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  mtry = tune(),
  loss_reduction = tune()
) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#Workflow
wf_xgb_tune2 <- workflow() |> 
  add_model(xgb_model) |> 
  add_recipe(churn_recipe)

#Nova Grade de Hiperpar√¢metros
grid2 <- grid_space_filling(
  finalize(mtry(), train),
  trees(),                         
  tree_depth(range = c(2, 10)),
  learn_rate(range = c(-5, -1)),
  loss_reduction(),
  size = 20)

#ReExecu√ß√£o
tuned_xgb2 <- tune_grid(
  wf_xgb_tune2,
  resamples = folds,     # valida√ß√£o cruzada
  grid = grid2,
  metrics = metric_set(roc_auc, accuracy, sens, spec),
  control = control_grid(save_pred = TRUE)
)

#Vendo combina√ß√µes dos 10 melhores modelos depois da reexecu√ß√£o
show_best(tuned_xgb2, metric = "sens", n = 10)

# üìä Interpreta√ß√£o dos 10 melhores modelos
# A sensibilidade dos 10 melhores est√° entre 0.670 e 0.676.
# Isso √© excelente: significa que o modelo atingiu um patamar est√°vel.
# Vamos olhar os padr√µes.
# 
# üå≥ 1. mtry variando de 1 a 10
# Isso mostra que:
# ‚Ä¢ 	o modelo n√£o depende fortemente de um n√∫mero espec√≠fico de vari√°veis por split
# ‚Ä¢ 	v√°rias combina√ß√µes funcionam bem
# ‚Ä¢ 	isso √© t√≠pico quando as vari√°veis t√™m relev√¢ncia distribu√≠da
# 
# üå≤ 2. trees variando de 211 a 2000
# Isso indica:
# ‚Ä¢ 	modelos com muitas √°rvores continuam sendo competitivos
# ‚Ä¢ 	mas modelos menores (ex.: 316, 421, 527) tamb√©m funcionam bem
# ‚Ä¢ 	o learning rate controla o ritmo de aprendizado
# 
# üß† 3. tree_depth entre 3 e 10
# Isso √© √≥timo:
# ‚Ä¢ 	profundidades moderadas ‚Üí menos overfitting
# ‚Ä¢ 	profundidades altas (8‚Äì10) aparecem entre os melhores
# ‚Ä¢ 	profundidades baixas (3‚Äì4) tamb√©m aparecem
# Ou seja: o modelo est√° flex√≠vel, mas n√£o exagerado.
# 
# üê¢ 4. learn_rate agora est√° dentro do intervalo realista
# Valores como:
# ‚Ä¢ 	0.000113
# ‚Ä¢ 	0.00001
# ‚Ä¢ 	0.00546
# ‚Ä¢ 	0.000785
# ‚Ä¢ 	0.0144
# ‚Ä¢ 	0.1
# Isso √© perfeito:
#   o modelo est√° explorando desde learning rates lentos at√© r√°pidos, sem cair em extremos absurdos.
# 
# üîß 5. loss_reduction variando muito
# Isso √© esperado:
# ‚Ä¢ 	valores pequenos ‚Üí splits mais agressivos
# ‚Ä¢ 	valores grandes ‚Üí splits mais conservadores
# ‚Ä¢ 	ambos aparecem entre os melhores
# Isso mostra que o modelo est√° encontrando boas solu√ß√µes em diferentes regimes de complexidade.
# 

#üèÜSelecionando o melhor modelo e rodando o treino inteiro nele
# 1. 	tune_grid() ‚Üí gera v√°rias combina√ß√µes
# 2. 	select_best() ‚Üí retorna um tibble com os hiperpar√¢metros
# 3. 	finalize_workflow() ‚Üí coloca esses hiperpar√¢metros dentro do workflow
# 4. 	fit() ‚Üí treina o modelo final

best_model <- select_best(tuned_xgb2, metric = "sens")
best_model

final_wf2 <- finalize_workflow(wf_xgb_tune2, best_model)
final_fit2 <- fit(final_wf2, data = train)

pred_test_final <- predict(final_fit2, new_data = test, type = "prob") |> 
  bind_cols(predict(final_fit2, new_data = test, type = "class")) |> 
  bind_cols(test |> select(churn))

test_metrics(pred_test_final, truth = churn, estimate = .pred_class)
# Abaixo a analise das m√©tricas:
# üéØ 1. Sensibilidade = 0.671
# Esse √© o ponto central, j√° que voc√™ otimizou o modelo para sensibilidade.
# ‚Ä¢ 	O modelo est√° capturando 2 de cada 3 clientes que realmente churnam.
# ‚Ä¢ 	Isso √© muito bom para um problema de churn, especialmente se a classe positiva for pequena.
# ‚Ä¢ 	E, o mais importante:esse valor √© consistente com o que o tuning encontrou (0.67‚Äì0.68).
# Ou seja:
#   üëâ o modelo generalizou bem
#   üëâ n√£o houve overfitting
#   üëâ o tuning funcionou
# 
# üõ°Ô∏è 2. Especificidade = 0.935
# Isso significa:
# ‚Ä¢ 	O modelo quase n√£o gera falsos positivos.
# ‚Ä¢ 	Ele identifica corretamente 93,5% dos clientes que n√£o churnam.
# Esse equil√≠brio √© √≥timo: aumentou recall sem destruir a capacidade de prever ‚Äún√£o churn‚Äù.
# 
# üß† 3. Accuracy = 0.845
# Esse valor √© praticamente igual ao do modelo anterior (0.843), mas agora:
# ‚Ä¢ 	com tuning mais est√°vel
# ‚Ä¢ 	sem hiperpar√¢metros extremos
# ‚Ä¢ 	com sensibilidade melhor calibrada
# ganhou qualidade, n√£o apenas n√∫meros.

#√Årea sob a curva ROC
pred_test_final |>  roc_auc(truth = churn, .pred_Yes, event_level = "first")

#Matriz de confus√£o
conf_mat(pred_test_final, truth = churn, estimate = .pred_class)

#Curvas de densidade de probabilidades
pred_test_final |> ggplot(aes(.pred_Yes, fill = churn))+
geom_density(alpha = 0.4)

#Curva de lift/Gain
gain_curve(pred_test_final, truth = churn, .pred_Yes) |>  autoplot()

#üìà AUC = 0.806
# Um AUC de 0.806 significa que:
# ‚Ä¢ 	o modelo tem boa capacidade discriminativa
# ‚Ä¢ 	separa bem clientes que churnam dos que n√£o churnam
# ‚Ä¢ 	est√° acima do patamar t√≠pico de modelos baseline (0.60‚Äì0.70)
# Em churn, AUC acima de 0.80 j√° √© considerado muito bom.
# Isso confirma que:
# ‚Ä¢ 	o tuning funcionou
# ‚Ä¢ 	o modelo generaliza bem
# ‚Ä¢ 	n√£o houve overfitting
# üéØ 1. Verdadeiros Positivos (TP) = 919
# Clientes que churnaram e o modelo acertou.
# Isso corresponde √† sensibilidade de 0.671, exatamente o que voc√™ j√° viu.
# 
# üõë 2. Falsos Negativos (FN) = 450
# Clientes que churnaram, mas o modelo previu ‚ÄúNo‚Äù.
# Esse √© o grupo que voc√™ tenta reduzir quando otimiza sensibilidade.
# O tuning ajudou a diminuir esse n√∫mero sem sacrificar muito a precis√£o.
# 
# üü© 3. Verdadeiros Negativos (TN) = 2460
# Clientes que n√£o churnaram e o modelo acertou.
# Isso corresponde √† especificidade de 0.935 ‚Äî excelente.
# 
# üü® 4. Falsos Positivos (FP) = 172
# Clientes que n√£o churnaram, mas o modelo previu ‚ÄúYes‚Äù.

# Esse n√∫mero √© baixo, o que √© √≥timo para evitar a√ß√µes desnecess√°rias (ex.: oferecer desconto para quem n√£o ia sair).
